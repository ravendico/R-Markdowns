---
title: '__Statistical Modeling Course__'
subtitle: '__Variable Selection Assignment__'
output:
  pdf_document:
    fig_height: 3
    fig_width: 4.5
    extra_dependencies: ["xcolor"]
header-includes:
  - \usepackage{setspace}\onehalfspacing
fontsize: 11pt
---

\definecolor{blue}{HTML}{0066f5}


In this exercise, we will predict the number of applications, `Apps`, received using the other variables in the \color{blue} `College` \color{black} data set. Use 5-fold cross-validation to calculate the test error using the following methods. You should fit each model five times leaving out one fifth of the data each time and calculating the test error (RMSE) on the data you left out. The total test error will be the average RMSE from the five folds. You should also calculate the standard error of the test errors, `sd(test_error)/sqrt(5)`. For best subset selection, and forward stepwise selection chose the model using BIC. For lasso, ridge, and pcr you will conduct a second level of cross validation for each of the 5 training sets. You can do this with `cv.glmnet`. 

Compare your results with the null model. Code for the null model and splits is given below, make sure to compare all of your models on the same splits of the data.

```{r, include = FALSE}
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
```
### Null Model
```{r}
set.seed(2020)
# Vector of data split
folds <- sample(1:5, nrow(College), replace = TRUE)

# Variable to store error from each fold
error_null <- c()

# Loop through folds and fit models with no predictors
for (i in 1:5){
  test <- College[folds == i, ]
  train <- College[folds != i, ]
  error_null[i] <- sqrt(mean((test$Apps - mean(train$Apps))^2))
}

# Print mean RMSE
mean(error_null)

# Print standard error of RMSE
sd(error_null)/sqrt(5)
```

# Problem 1
- Least squares linear model
```{r}
set.seed(2020)
# Vector of data split
folds <- sample(1:5, nrow(College), replace = TRUE)

# Variable to store error from each fold
error_null <- c()

# Loop through folds and fit models with no predictors
for (i in 1:5){
  test <- College[folds == i, ]
  train <- College[folds != i, ]
  lst_sqrs_mod <- lm(Apps~., data= train)
  pred <- predict(lst_sqrs_mod, newdata = test)
  error_null[i] <- sqrt(mean((test$Apps - pred)^2))
}

# Print mean RMSE
mean(error_null)

# Print standard error of RMSE
sd(error_null)/sqrt(5)
```

- Best subset selection (chosen using BIC)
```{r}
#Creating predict function for subset selection models
predict.regsubsets <- function(object,newdata,id,...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form,newdata)
  coefi = coef(object,id=id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}

set.seed(2020)
# Vector of data split
folds <- sample(1:5, nrow(College), replace = TRUE)

# Variable to store error from each fold
error_null <- c()

# Loop through folds and fit models with no predictors
for (i in 1:5){
  test <- College[folds == i, ]
  train <- College[folds != i, ]
  regfit.full = regsubsets(Apps~., train, nvmax= 17)
  
  for (i in 1:19){
    pred <- predict(regfit.full, newdata = test, id=i)
    cv
  error_null[i] <- sqrt(mean((test$Apps - pred)^2))
}


# Print mean RMSE
mean(error_null)

# Print standard error of RMSE
sd(error_null)/sqrt(5)
```

- Forward stepwise subset selection (chosen using  BIC)

```{r}


regfit.full = regsubsets(Apps~., College, nvmax= 17)
regfit.forward = regsubsets(Apps~., College, nvmax= 17, method="forward")

nvar_full = which.min(summary(regfit.full)$bic)
nvar_forward = which.min(summary(regfit.forward)$bic)

tibble(nvar_full, nvar_forward)
coef(regfit.full, nvar_full)
coef(regfit.forward, nvar_forward)

plot(regfit.full ,scale ="bic")
plot(regfit.forward ,scale ="bic")
```

# Problem 2
- Ridge-regression with lambda chosen by cross-validation, report the five $\lambda$'s chosen
```{r}

```

- Lasso with lambda chosen by cross-validation, report the five $\lambda$'s chosen

```{r}

#glmnet: alpha = 0 <- ridge, alpha = 1 <- lasso,
grid = 10^seq(10, -2, length = 100)
x = model.matrix(Apps~., College)[,-1]
y = College$Apps

set.seed(2020)
# Vector of data split
folds <- sample(1:5, nrow(College), replace = TRUE)

# Variable to store error from each fold
error_null_ridge <- c()
error_null_lasso <- c()

# Loop through folds and fit models with no predictors
for (i in 1:5){
  x_test <- x[folds == i, ]
  x_train <- x[folds != i, ]
  y_test <- y[folds == i]
  y_train <- y[folds != i]
  ridge.mod = cv.glmnet(x_train, y_train, alpha=0, lambda=grid, thresh=1e-12)
  lasso.mod = cv.glmnet(x_train, y_train, alpha=1, lambda=grid, thresh=1e-12)
  ridge.pred = predict(ridge.mod, s=ridge.mod$lambda.min, newx = x_test)
  lasso.pred = predict(lasso.mod, s=lasso.mod$lambda.min, newx = x_test)
  error_null_ridge[i] <- sqrt(mean((y_test - ridge.pred)^2))
  error_null_lasso[i] <- sqrt(mean((y_test - lasso.pred)^2))
}

# Print mean RMSE
mean(error_null_ridge)
mean(error_null_lasso)

# Print standard error of RMSE
sd(error_null_ridge)/sqrt(5)
sd(error_null_lasso)/sqrt(5)
```


# Problem 3
- Fit a PCR model, with M chosen by cross-validation. Report the test error (MSE) obtained, along with the value of M selected by cross-validation.

```{r}
set.seed(2020)
# Vector of data split
folds <- sample(1:5, nrow(College), replace = TRUE)

# Variable to store error from each fold
error_null_PCR <- c()

# Loop through folds and fit models with no predictors
for (i in 1:5){
  test <- College[folds == i, ]
  train <- College[folds != i, ]
  PCR.mod <- pcr(Apps ~ ., data=train, scale=TRUE , validation ="CV")
  s = which.min(RMSEP(PCR.mod)$val[1,,])-1
  PCR.pred = predict(PCR.mod, ncomp=s, newx = x_test)
  error_null_PCR[i] <- sqrt(mean((test$Apps - PCR.pred)^2))
}

# Print mean RMSE
mean(error_null_PCR)

# Print standard error of RMSE
sd(error_null_PCR)/sqrt(5)

```


# Problem 4
Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from the approaches?



# Problem 5

Generate a data set with $p = 100$ features, $n = 300$ observations, and an associated quantitative response vector generated according to the model

$$p_i = \frac{e^{x_i^T\beta}}{1+e^{x_i^T\beta}}$$
$$y_i \sim Bin(p_i, n=1)$$

Simulate some of the features as categorical and some as numeric. Set most of the values of $\beta_p = 0$ for most but not all $p$. 

- Using your simulated dataset split your dataset into at training set and a test set containing using an 80/20 split. 
- Perform lasso and ridge regression on the training set.
- Which model has a lower test error (MSE)? Comment on your results. 

```{r}

```

