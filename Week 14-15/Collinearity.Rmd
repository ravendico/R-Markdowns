---
title: '__Statistical Modeling Course__'
subtitle: '__Collinearity Lab__'
output:
  pdf_document:
    fig_height: 3
    fig_width: 4.5
    extra_dependencies: ["xcolor"]
header-includes:
  - \usepackage{setspace}\onehalfspacing
fontsize: 11pt
---

```{r, echo = FALSE, include = FALSE}
library(tidyverse)
library(car)
```

\definecolor{blue}{HTML}{0066f5}

This lab focuses on the *collinearity* problem. Perform the following commands in \color{blue} `R` \color{black}. The last line corresponds to creating a linear model in which \color{blue} `y` \color{black} is a function of \color{blue} `x1` \color{black} and \color{blue} `x2`\color{black}.

```{r}
set.seed(1)
x1 = runif(100)
x2 = 0.5*x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
df = tibble(y, x1, x2)
```
    
## Problem 1
What is the correlation between \color{blue} `x1` \color{black} and \color{blue} `x2`\color{black}? What is the variance inflation factor? How about the condition number of $X^TX$?

```{r}
cor(df$x1,df$x2)
vif(lm(y~., data=df))
kappa(df[,-1])
```


## Problem 2
Using this data, fit a least squares regression to predict \color{blue} `y` \color{black} using  \color{blue} `x1` \color{black} and \color{blue} `x2`\color{black}. How do these relate to the true $\beta_0$, $\beta_1$, and $\beta_2$? Can you reject the null hypothesis $H_0: \beta_1 = 0$? How about the null hypothesis $H_0: \beta_2 = 0$?

```{r}
mod <- lm(y~., data=df)
summary(mod)
```

\textit{Answer:}
For the following model, we can reject the null hypothesis that B1=0 but we cannot reject the null hypothesis that B2=0.

## Problem 3
Now fit a least squares regression to predict \color{blue} `y` \color{black} using only \color{blue} `x1` \color{black}. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?

```{r}
mod2 <- lm(y~x1, data=df)
summary(mod2)
```

\textit{Answer:}
In this model, we can reject the null hypothesis that B1=0.

## Problem 4
Now fit a least squares regression to predict \color{blue} `y` \color{black} using only \color{blue} `x2` \color{black}. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?

```{r}
mod3 <- lm(y~x2, data=df)
summary(mod3)
```

\textit{Answer:}
In this model, we can reject the null hypothesis that B2=0.

## Problem 5
Do the results obtained in Problem 2 and 4 contradict each other? Explain your answer.

\textit{Answer: }

The results in Problem 2 and 4 show what happens when the predictor variables are highly correlated with each other. The results contradict each other. Since x1 and x2 have high correlation, then using them both in a model to predict y results in x2 having no statistically significant relationship with y. However, using them separately in two models results in each having significant linear relationship with y.



